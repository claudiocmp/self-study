{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Estimation\n",
    "\n",
    "## Maximum likelihood estimation\n",
    "\n",
    "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model, given observations. MLE attempts to find the parameter values that maximize the likelihood function, given the observations. The resulting estimate is called a maximum likelihood estimate, which is also abbreviated as MLE.\n",
    "The method of maximum likelihood is used with a wide range of statistical analyses. As an example, suppose that we are interested in the heights of adult female penguins, but are unable to measure the height of every penguin in a population (due to cost or time constraints). Assuming that the heights are normally distributed with some unknown mean and variance, the mean and variance can be estimated with MLE while only knowing the heights of some sample of the overall population. MLE would accomplish that by taking the mean and variance as parameters and finding particular parametric values that make the observed results the most probable given the normal model.\n",
    "From the point of view of Bayesian inference, MLE is a special case of maximum a posteriori estimation (MAP) that assumes a uniform prior distribution of the parameters. In frequentist inference, MLE is one of several methods to get estimates of parameters without using prior distributions. Priors are avoided by not making probability statements about the parameters, but only about their estimates, whose properties are fully defined by the observations and the statistical model.\n",
    "\n",
    "The hat over l indicates that it is akin to an estimator. Indeed, l estimates the expected log-likelihood of a single observation in the model\n",
    "Suppose one wishes to determine just how biased an unfair coin is. Call the probability of tossing a ‘head’ p. The goal then becomes to determine p.\n",
    "Suppose the coin is tossed 80 times: i.e. the sample might be something like x1 = H, x2 = T, ..., x80 = T, and the count of the number of heads \"H\" is observed.\n",
    "The probability of tossing tails is 1 − p (so here p is θ above). Suppose the outcome is 49 heads and 31 tails, and suppose the coin was taken from a box containing three coins: one which gives heads with probability p = ​1⁄3, one which gives heads with probability p = ​1⁄2 and another which gives heads with probability p = ​2⁄3. The coins have lost their labels, so which one it was is unknown. Using maximum likelihood estimation the coin that has the largest likelihood can be found, given the data that were observed. By using the probability mass function of the binomial distribution with sample size equal to 80, number successes equal to 49 but for different values of p (the \"probability of success\"), the likelihood function (defined below) takes one of three values:\n",
    "\n",
    "The likelihood is maximized when p = ​2⁄3, and so this is the maximum likelihood estimate for p.\n",
    " \n",
    "#### Discrete distribution, continuous parameter space\n",
    "\n",
    "Now suppose that there was only one coin but its p could have been any value 0 ≤ p ≤ 1. The likelihood function to be maximised is\n",
    "\n",
    "and the maximisation is over all possible values 0 ≤ p ≤ 1.\n",
    "One way to maximize this function is by differentiating with respect to p and setting to zero:\n",
    "\n",
    " \n",
    "which has solutions p = 0, p = 1, and p = ​49⁄80. The solution which maximizes the likelihood is clearly p = ​49⁄80 (since p = 0 and p = 1 result in a likelihood of zero). Thus the maximum likelihood estimator for p is ​49⁄80.\n",
    "This result is easily generalized by substituting a letter such as s in the place of 49 to represent the observed number of 'successes' of our Bernoulli trials, and a letter such as n in the place of 80 to represent the number of Bernoulli trials. Exactly the same calculation yields ​s⁄n which is the maximum likelihood estimator for any sequence of n Bernoulli trials resulting in s 'successes'.\n",
    "#### Continuous distribution, continuous parameter space[edit]\n",
    "For the normal distribution which has probability density function\n",
    "\n",
    "the corresponding probability density function for a sample of n independent identically distributed normal random variables (the likelihood) is\n",
    "\n",
    "or\n",
    "over both parameters simultaneously, or if possible, individually.\n",
    "Since the logarithm function itself is a continuous strictly increasing function over the range of the likelihood, the values which maximize the likelihood will also maximize its logarithm (the log-likelihood itself is not necessarily strictly increasing). The log-likelihood can be written as follows:\n",
    "\n",
    "(Note: the log-likelihood is closely related to information entropy and Fisher information.)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "## Chi-square estimation\n",
    "Suppose a certain random variable takes values in the set of non-negative integers 1, 2, 3, . . . . A simple random sample of size 20 is taken, yielding the following data set. It is desired to test the null hypothesis that the population from which this sample was taken follows a Poisson distribution.\n",
    "\n",
    "\n",
    "\n",
    "The maximum likelihood estimate of the population average is 3.3. One could apply Pearson's chi-square test of whether the population distribution is a Poisson distribution with expected value 3.3. However, the null hypothesis did not specify that it was that particular Poisson distribution, but only that it is some Poisson distribution, and the number 3.3 came from the data, not from the null hypothesis. A rule of thumb says that when a parameter is estimated, one reduces the number of degrees of freedom by 1, in this case from 9 (since there are 10 cells) to 8. One might hope that the resulting test statistic would have approximately a chi-square distribution when the null hypothesis is true. However, that is not in general the case when maximum-likelihood estimation is used. It is however true asymptotically when minimum chi-square estimation is used.\n",
    "\n",
    "The minimum chi-square estimate of the population mean λ is the number that minimizes the chi-square statistic\n",
    "\n",
    "\n",
    "where a is the estimated expected number in the \"> 8\" cell, and \"20\" appears because it is the sample size. The value of a is 20 times the probability that a Poisson-distributed random variable exceeds 8, and it is easily calculated as 1 minus the sum of the probabilities corresponding to 0 through 8. By trivial algebra, the last term reduces simply to a. Numerical computation shows that the value of λ that minimizes the chi-square statistic is about 3.5242. That is the minimum chi-square estimate of λ. For that value of λ, the chi-square statistic is about 3.062764. There are 10 cells. If the null hypothesis had specified a single distribution, rather than requiring λ to be estimated, then the null distribution of the test statistic would be a chi-square distribution with 10 − 1 = 9 degrees of freedom. Since λ had to be estimated, one additional degree of freedom is lost. The expected value of a chi-square random variable with 8 degrees of freedom is 8. Thus the observed value, 3.062764, is quite modest, and the null hypothesis is not rejected.\n",
    " \n",
    "\n",
    "\n",
    "## Partial least squares\n",
    "\n",
    "## Probability calibration\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
