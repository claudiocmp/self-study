{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "## Probability distributions\n",
    "\n",
    "Bernoulli: distribution over a binary random variable\n",
    "P(x=1) = Φ\n",
    "P(x=0) = 1 - Φ\n",
    "Where Φ[0,1] is the parameter of the distribution.\n",
    "Multinoulli (categorical distribution): distribution over a random variable with k possible states (k finite).\n",
    "Gaussian: normal distribution (if xR) or multivariate normal distribution (if xRn)\n",
    "Exponential: It presents a sharp point at x=0, and P(x<0) = 0\n",
    "Laplace: It presents a sharp peak of probability mass at an arbitrary point \n",
    "Dirac: It has probability 1 at one point, and 0 at all other points\n",
    "Empirical: It has probability 1/m at each point forming a given collection of samples. It is necessary only for defining empirical distribution of continuous variables (for discrete variables, the probability is associated to the empirical frequency of each state throughout the collection of samples).\n",
    "\n",
    "## Probability density function and probability\n",
    "\n",
    "Probability density functions represent how a random variable (i.e. a characteristic) is expressed in a population. Then, we can calculate the probability for a sample of being within given interval. \n",
    "Example 1: Normal distribution\n",
    "A population has mean weight of 180 pounds, with a std dev of 34 pounds. Calculate the probability of an individual being c1>120 and c2<155 pounds.\n",
    "\n",
    "\n",
    "\n",
    "P(Y>120 Y<155)=122c1c2e-(x-)222\n",
    "So we normalize to normal distribution by a factor z=x-, so the constants become z1=c1-and z2=c2- and then split the integral in two:\n",
    "\n",
    "12(-z2e-12z2 --z1e-12z2 )\n",
    "\n",
    "z1=-1.764\n",
    "z2=-0.735\n",
    "\n",
    "Use of table. Here, we use this table which gives probability from the mean, whereas our integrals are expressed from -inf. Just do the right conversions.\n",
    "\n",
    "P(Z=1.764) = 0.4608 ~ 0.4616 => 0.4612\n",
    "P(Z=0.735) = 0.2673 ~ 0.2704 => 0.2688\n",
    "\n",
    "P(Y>120 Y<155) =46.12% -26.88% =  19.24% \n",
    "Example 2: Binomial distribution\n",
    "Assume that 15% of the population is allergic to cats. If you randomly select 60 people for a medical trial, what is the probability that 7 of those people are allergic to cats?\n",
    "\n",
    "Just to know, we calculate the mean and standard dev of the probability density function:\n",
    "\n",
    "$\\mu =p*n=0.15*60=9.0$\n",
    "\n",
    "$\\sigma = \\sqrt{p*n*(1-p)}=0.15*60*(1-0.5)=2.121$\n",
    "\n",
    "So, the probability density function will be:\n",
    "[n,k]*pk*(1-p)n-k\n",
    "Where [n,k] is\n",
    "n!k!(n-k)!\n",
    "And so:\n",
    "60!7!(60-7)!*0.157*(1-0.15)60-7=11.98\n",
    "\n",
    "## Cumulative density function\n",
    "\n",
    "TODO\n",
    "\n",
    "## p-value\n",
    "The p-value is defined as the probability of obtaining a result equal to or more extreme than what was actually observed.\n",
    "\n",
    "![](img/Page-2-Image-1.png)\n",
    "\n",
    "This must happen under the null hypothesis, here simply denoted by H (but is often denoted H0, as opposed to Ha, which is sometimes used to represent the alternative hypothesis), \n",
    "Depending on how it is looked at, the \"more extreme than what was actually observed\" can mean:\n",
    "\n",
    "|Even type|Random variable condition|p-value|\n",
    "|---|---|---|\n",
    "|Right tail|{Xx}|Pr(Xx|H)|\n",
    "|Left tail|{Xx}|Pr(Xx|H)|\n",
    "|Double-tailed event|argmin({Xx},{Xx})|2*argmin{Pr(Xx|H),Pr(Xx|H)}|\n",
    "\n",
    "**NOTE**: The smaller the p-value, the higher the significance because it tells the investigator that the hypothesis under consideration may not adequately explain the observation.\n",
    "\n",
    "**WARNING**: Pr(observation | H) Pr(H | observation) Therefore, using the p-value as a score (for a hypothesis) is WRONG\n",
    "\n",
    "## Central Limit Theorem\n",
    "\n",
    "TODO\n",
    "\n",
    "## Mixture of distributions\n",
    "\n",
    "Given a collection of samples we can:\n",
    "Know the components in advance (prior probability)\n",
    "The components of the population are unknown (posterior approximator) In this case, we use MM for clustering (unsupervised learning) purposes, where component = cluster.\n",
    "\n",
    "### Mixture Distribution\n",
    "One way to combine distributions is to use a multinoulli distribution to pick up a base distribution to generate the sample.\n",
    "P(x)=P(c=i)P(x | c=i)\n",
    "Where P(c) is the multinoulli distribution over the components.\n",
    "E.g. empirical distribution is a mixture distribution with one Dirac component for each sample.\n",
    "\n",
    "### Gaussian mixture model\n",
    "\n",
    "Ref: [colab-notebook](https://jakevdp.github.io/PythonDataScienceHandbook/05.12-gaussian-mixtures.html)\n",
    "\n",
    "Each component P(x | c=i) is a gaussian with separately parametrized mean  and covariance . However, more constraints could be introduced (e.g. covariance might be shared between all components, but not the mean).\n",
    "\n",
    "#### Example 1\n",
    "prices of books are influenced by paperback and hardback cover:\n",
    "![](img/Page-4-Image-2.png)![](img/Page-4-Image-3.png)\n",
    "\n",
    "#### Example 2\n",
    "Here is how GMM avoid the main problem of K-means (related to the distance from the mean (or centre of cluster)\n",
    "![](img/Page-4-Image-4.png)![](img/Page-4-Image-5.png)\n",
    "\n",
    "However, figure on the right makes clear of a problematic of GMM: how many components? SciKit-Learn’s GMM includes two ways of evaluating the quality of the statistical model:\n",
    "\n",
    "* Akaike information criterion (AIC), check if the model describe the reality\n",
    "* the Bayesian information criterion (BIC), assumes the reality and select n models\n",
    "\n",
    "For the figure on the right, we plot both. The min+ value of the curves indicate a good number of components (clusters) - i.e. for this case 8-10. [Here](https://stats.stackexchange.com/questions/577/is-there-any-reason-to-prefer-the-aic-or-bic-over-the-other) for the method to prefer.\n",
    "\n",
    "![](img/Page-4-Image-6.png)\n",
    "\n",
    "#### Example 3\n",
    "From www.deeplearningbook.org:\n",
    "\n",
    "![](img/Page-5-Image-7.png)\n",
    "\n",
    "## Bayes Theorem\n",
    "Given the tree diagram (right) and the superposition event figure (left), see the pathAAB and BAB. It shows that:\n",
    "\n",
    "$P(A|B)P(B) = AB =P(B|A)P(A) $\n",
    "\n",
    "![](img/Page-5-Image-8.png) ![](img/Page-5-Image-9.png)\n",
    "\n",
    "Which is represented on the AB plane as (be careful, the chart is not 100% correct):\n",
    "\n",
    "![](img/Page-6-Image-10.png)\n",
    "\n",
    "Therefore,  P(A|B)=P(B|A)P(A)P(B) where P(A), P(B) are the residual probabilities.\n",
    "\n",
    "## Null Hypothesis\n",
    "\n",
    "The null hypothesis is a general statement or default position that there is nothing new happening, like there is no association among groups, or no relationship between two measured phenomena.\n",
    "\n",
    "## Technical Details of Continuous Variables\n",
    "\n",
    "### Measure Theory\n",
    "The discipline that synthesise the formal understanding of continuous random variables and probability density functions. It is useful for describing theorems that apply to most points in Rn but do not apply to some corner cases (reasonably small sets).\n",
    "\n",
    "A measure on a set is a systematic way to assign a number to each suitable subset of that set, intuitively interpreted as its size. Definition of a measure: a function that assigns a non-negative real number or +∞ to (certain) subsets of a set X (see Definition below). It must further be countably additive. This means that countable unions, countable intersections and complements of measurable subsets are measurable.\n",
    "\n",
    "### Paradox\n",
    "\n",
    "It is possible to construct two sets S1 and S2 such that \n",
    "p(x ∈ S1) + p(x ∈ S2) > 1 \n",
    "S1 ∩  S2 = ∅\n",
    "Change of variable\n",
    "Given the two random variables x and y, such that y = g(x), where g is invertible, continuous and derivable, we have that:\n",
    "py(y)  px(g-1(y))\n",
    "The non equality is because g(x) would distort the space (domain) of y, for a given x.\n",
    "\n",
    "[...] Change of variable\n",
    "\n",
    "\n",
    "## Structured Probabilistic Models\n",
    "\n",
    "Are combined models which represents multiple dependencies between random variables, represented by a graph, where:\n",
    "* Node = random variable\n",
    "* Edge =  represent direct interactions between the two connected random variables\n",
    "Then, graphs can be:\n",
    "* Directed, where edges represent factorization of conditional probability distributions.\n",
    "* Undirected, where edges represent factorization of a set of functions, which are not probability distributions (see below for details)\n",
    "SPMs are a language for describing probability distributions. **Being directed or undirected** is not a property of a probability distribution (are not mutually exclusive families of probability distributions). **It is a property of a particular description of a probability distribution, but any probability distribution may be described in both ways.**\n",
    "\n",
    "### Directed model\n",
    "\n",
    "It contains one factor for every random variable xi in the distribution, and that factor consists of the conditional distribution over xi given the parents of xi, denoted Pa G(xi). Therefore we have the product of sequence (Italian: produttoria):\n",
    "p(x) = 0n p ( xi | Pa G(xi) )\n",
    "Which basically is the product of the concatenated conditional probability (chain rule).\n",
    "![](img/Page-8-Image-11.png)\n",
    "\n",
    "### Undirected model\n",
    "It contains one factor for every edge; undirected models are studied per couple of nodes connected by an edge, called Cliques C(i)- the basic undirected model. Each clique is associated with a factor, which is a function (not a probability one) denoted (i)(C(i)), where:\n",
    "Codomain of (i)is positive\n",
    "The integral of (i)can be diverse from 1 (unlike a probability function)\n",
    "The probability of a conﬁguration of random variables is proportional to the product of all these factors:\n",
    "p(x)    (i)\n",
    "\n",
    "Because of there is no guarantee that this product will sum to 1. Therefore, we normalize by dividing by Z (the sum or integral over all states of the product of the   functions - or upper limit), in order to obtain a normalized probability distribution:\n",
    "p(x) = 1Z0n(i)(C(i))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
